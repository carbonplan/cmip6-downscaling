{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ccf2fa1-24da-46d9-a0ea-62cb9e0b21fc",
   "metadata": {},
   "source": [
    "# Inference with DeepSD models\n",
    "\n",
    "This notebook runs inference with DeepSD models trained using\n",
    "`deepsd_model_train.ipynb`. Here, it is assumed that the stacked model are\n",
    "stored on Azure with paths conforming to `stacked_model_path` and\n",
    "`output_node_name` templates defined in `deepsd.py`.\n",
    "\n",
    "The input data for inference are:\n",
    "\n",
    "1. the interpolated, normalized GCM data (i.e. z-score by the historical period\n",
    "   of GCM),\n",
    "2. elevation data of all resolutions within the stacked model.\n",
    "\n",
    "Different GCMs are bilinearly interpolated to either 2 degree or 1 degree,\n",
    "before being fed into the DeepSD model (see `deepsd_data_prep.ipynb` for more\n",
    "details about how we determine the initial resolutions. The input data are maps\n",
    "of the entire spatial domain instead of patches as in training.\n",
    "\n",
    "Due to memory constraints, model inference is performed and saved in batches.\n",
    "Note that the inference code as is _does not_ look for existing data. Thus,\n",
    "running that block of code is time consuming and will overwrite previously\n",
    "inferenced data.\n",
    "\n",
    "The raw output from DeepSD models are the normalized values. We then use the\n",
    "mean and standard deviation of historical observation data to restore the\n",
    "output. This step can be viewed as a bias correction step.\n",
    "\n",
    "**Note**: This notebook has been superceded by the DeepSD prefect flow\n",
    "`flows/methods/deepsd/flow.py`. The interpolation and standardization order\n",
    "differs between this notebook and the flow.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da67b2e7-b40b-47e3-b73c-d002de4709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894a9707-d5ef-426c-a838-8e4a2c1c1f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import xesmf as xe\n",
    "import intake\n",
    "import dask\n",
    "import fsspec\n",
    "import xbatcher\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import tensorflow_io\n",
    "\n",
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bf9c4f7-a8b7-4cf8-a18a-ca00db06737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.get_filesystem_class(\"az\")(\n",
    "    account_name=\"carbonplan\", account_key=os.environ[\"TF_AZURE_STORAGE_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9ff71d-abfb-495c-b53e-d9a99a4df5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine which GCM/params to run\n",
    "gcm = \"CanESM5\"\n",
    "scenario = \"ssp370\"\n",
    "\n",
    "# variable to predict on\n",
    "var = \"pr\"\n",
    "\n",
    "# the historical period used for normalization\n",
    "historical_period_start = \"1981\"\n",
    "historical_period_end = \"2010\"\n",
    "\n",
    "# the inference period, can overlap with historical period\n",
    "inference_period_start = \"1850\"\n",
    "inference_period_end = \"2014\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b115f2-ffcd-4da6-8ae6-54c4184d6000",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_downscaling.data.cmip import get_gcm, load_cmip\n",
    "from cmip6_downscaling.data.observations import get_obs\n",
    "from cmip6_downscaling.methods.deepsd import (\n",
    "    bilinear_interpolate,\n",
    "    starting_resolutions,\n",
    "    EPSILON,\n",
    "    stacked_model_path,\n",
    "    output_node_name,\n",
    "    res_to_str,\n",
    ")\n",
    "from cmip6_downscaling.workflows.utils import lon_to_180\n",
    "from cmip6_downscaling.methods.deepsd import (\n",
    "    get_elevation_data,\n",
    "    normalize,\n",
    "    build_grid_spec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ae6546-e3ad-435d-b5f8-1261a52be3ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_normalized_gcm_data(\n",
    "    gcm,\n",
    "    scenario,\n",
    "    var,\n",
    "    historical_period_start,\n",
    "    historical_period_end,\n",
    "    inference_period_start,\n",
    "    inference_period_end,\n",
    "):\n",
    "    # load all gcm data for the gcm/scenario/var requested\n",
    "    historical_gcm = load_cmip(\n",
    "        activity_ids=\"CMIP\",\n",
    "        experiment_ids=\"historical\",\n",
    "        source_ids=gcm,\n",
    "        variable_ids=[var],\n",
    "        return_type=\"xr\",\n",
    "    )\n",
    "\n",
    "    future_gcm = load_cmip(\n",
    "        activity_ids=\"ScenarioMIP\",\n",
    "        experiment_ids=scenario,\n",
    "        source_ids=gcm,\n",
    "        variable_ids=[var],\n",
    "        return_type=\"xr\",\n",
    "    )\n",
    "    # concat and convert to 180 degrees\n",
    "    ds_gcm = xr.combine_by_coords([historical_gcm, future_gcm], combine_attrs=\"drop_conflicts\")\n",
    "    ds_gcm = lon_to_180(ds_gcm)\n",
    "\n",
    "    # subset to the period of interest\n",
    "    historical_gcm = ds_gcm.sel(time=slice(historical_period_start, historical_period_end))\n",
    "    inference_gcm = ds_gcm.sel(time=slice(inference_period_start, inference_period_end))\n",
    "\n",
    "    # calculate the historical average and mean for z scoring\n",
    "    historical_gcm_mean = historical_gcm.mean(dim=\"time\").compute()\n",
    "    historical_gcm_std = historical_gcm.std(dim=\"time\").compute()\n",
    "\n",
    "    # bilinearly interpolated the inference data and the historical mean/std to the starting resolution\n",
    "    # this is 2.0 degrees for some GCMs and 1.0 degrees for others\n",
    "    starting_resolution = starting_resolutions[gcm]\n",
    "    inference_gcm = bilinear_interpolate(output_degree=starting_resolution, ds=inference_gcm)\n",
    "    historical_gcm_mean = bilinear_interpolate(\n",
    "        output_degree=starting_resolution, ds=historical_gcm_mean\n",
    "    )\n",
    "    historical_gcm_std = bilinear_interpolate(\n",
    "        output_degree=starting_resolution, ds=historical_gcm_std\n",
    "    )\n",
    "\n",
    "    # calculate zscore\n",
    "    return (inference_gcm - historical_gcm_mean) / (historical_gcm_std + EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ed95cd-68a6-45b1-b5fa-b8c0327858ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find all the output resolution for each SRCNN in the stacked model according to the starting resolution of the GCM of interest\n",
    "if starting_resolutions[gcm] == 2.0:\n",
    "    output_resolutions = [0.25, 0.5, 1.0]\n",
    "elif starting_resolutions[gcm] == 1.0:\n",
    "    output_resolutions = [0.25, 0.5]\n",
    "else:\n",
    "    raise Error(\"needs to be either 2.0 or 1.0\")\n",
    "\n",
    "# make sure this is from low res to high res\n",
    "output_resolutions = sorted(output_resolutions, reverse=True)\n",
    "\n",
    "# get elevations at all relevant resolutions\n",
    "elevs = []\n",
    "for output_res in output_resolutions:\n",
    "    elev = get_elevation_data(output_res)\n",
    "    elev_norm = normalize(ds=elev, dims=[\"lat\", \"lon\"], epsilon=EPSILON).elevation.values\n",
    "    elevs.append(tf.constant(elev_norm[np.newaxis, :, :, np.newaxis].astype(np.float32)))\n",
    "\n",
    "input_map = {\"elev_%i\" % i: elevs[i] for i in range(len(output_resolutions))}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884e5cca-7d25-48a5-9e54-7f3d7d14d171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now read in the frozen graph of the stacked model, set placeholder for x, constant for elevs\n",
    "x = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None, 1))\n",
    "input_map[\"lr_x\"] = x\n",
    "\n",
    "model_path = stacked_model_path.format(\n",
    "    var=var, starting_resolution=res_to_str(starting_resolutions[gcm])\n",
    ")\n",
    "output_node = output_node_name.format(var=var)\n",
    "\n",
    "with tf.io.gfile.GFile(model_path, \"rb\") as f:\n",
    "    graph_def = tf.compat.v1.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "    (y,) = tf.import_graph_def(\n",
    "        graph_def,\n",
    "        input_map=input_map,\n",
    "        return_elements=[output_node],\n",
    "        name=\"deepsd\",\n",
    "        op_dict=None,\n",
    "        producer_op_list=None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f6c675-fc5c-4179-9d38-e310f56f6f6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# now read the gcm model data\n",
    "gcm_norm = get_normalized_gcm_data(\n",
    "    gcm,\n",
    "    scenario,\n",
    "    var,\n",
    "    historical_period_start,\n",
    "    historical_period_end,\n",
    "    inference_period_start,\n",
    "    inference_period_end,\n",
    ")\n",
    "gcm_norm = gcm_norm.transpose(\"time\", \"lat\", \"lon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877a7b5-8375-4c20-8281-7fccfcc26698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_empty_dataset(lats, lons, times, output_path, var):\n",
    "    # if zarr already exists then just return the path and don't touch it since\n",
    "    # it will delete the existing store if you try to initialize again\n",
    "    if fs.exists(output_path):\n",
    "        pass\n",
    "\n",
    "    # if not then make an empty dataset\n",
    "    else:\n",
    "        ds = xr.DataArray(\n",
    "            np.empty(shape=(len(times), len(lats), len(lons))),\n",
    "            dims=[\"time\", \"lat\", \"lon\"],\n",
    "            coords=[times, lats, lons],\n",
    "        )\n",
    "        ds = ds.to_dataset(name=var)\n",
    "\n",
    "        mapper = fsspec.get_mapper(output_path)\n",
    "        ds.astype(\"float32\").to_zarr(mapper, mode=\"w\", compute=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c941dae-4a7c-4930-8eb3-af14670af488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# downscale data in batches and save to the output path\n",
    "batch_size = 5000\n",
    "n = len(gcm_norm.time.values)\n",
    "\n",
    "print(\"getting elevation\")\n",
    "output_path = f\"az://flow-outputs/results/deepsd/{gcm}_{scenario}_{historical_period_start}_{historical_period_end}_{inference_period_start}_{inference_period_end}_{var}_norm.zarr\"\n",
    "# use the 0.25 degree resolution as the template for lat/lon grid\n",
    "elev_hr = get_elevation_data(0.25)\n",
    "\n",
    "print(\"initializing\")\n",
    "initialize_empty_dataset(\n",
    "    lats=elev_hr.lat.values,\n",
    "    lons=elev_hr.lon.values,\n",
    "    times=gcm_norm.time.values,\n",
    "    output_path=output_path,\n",
    "    var=var,\n",
    ")\n",
    "\n",
    "print(\"batching\")\n",
    "for start in range(0, n, batch_size):\n",
    "    stop = min(start + batch_size, n)\n",
    "    print(start, stop)\n",
    "\n",
    "    X = gcm_norm.isel(time=slice(start, stop))[var].values\n",
    "\n",
    "    downscaled_batch = np.empty(\n",
    "        shape=(stop - start, len(elev_hr.lat.values), len(elev_hr.lon.values))\n",
    "    )\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        for i in range(X.shape[0]):\n",
    "            _x = X[i][np.newaxis, :, :, np.newaxis]\n",
    "            _y = sess.run(y, feed_dict={x: _x})\n",
    "            downscaled_batch[i, :, :] = _y[0, :, :, 0]\n",
    "\n",
    "    downscaled_batch = xr.DataArray(\n",
    "        downscaled_batch,\n",
    "        dims=[\"time\", \"lat\", \"lon\"],\n",
    "        coords=[\n",
    "            gcm_norm.isel(time=slice(start, stop)).time.values,\n",
    "            elev_hr.lat.values,\n",
    "            elev_hr.lon.values,\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    region = {\n",
    "        \"lat\": slice(0, len(elev_hr.lat.values)),\n",
    "        \"lon\": slice(0, len(elev_hr.lon.values)),\n",
    "        \"time\": slice(start, stop),\n",
    "    }\n",
    "\n",
    "    store = fsspec.get_mapper(output_path)\n",
    "    task = downscaled_batch.to_dataset(name=var).to_zarr(\n",
    "        store,\n",
    "        mode=\"a\",\n",
    "        region=region,\n",
    "        compute=False,\n",
    "    )\n",
    "    task.compute(retries=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec124e5-c03e-4f0e-8d06-ee08c65b44e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fsspec.get_mapper(output_path)\n",
    "downscaled_norm = xr.open_zarr(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a13b3a10-76c8-4212-9a4f-168c9014012e",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled_norm.isel(time=100)[var].plot(robust=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e2b88f-5045-4985-af9c-0f787866026d",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1fbce9-674a-41c5-8c08-0c53314409d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_downscaling.methods.deepsd import get_obs_mean, get_obs_std\n",
    "\n",
    "obs = \"ERA5\"\n",
    "variables = [\"pr\", \"tasmax\", \"tasmin\"]\n",
    "gcm_grid_spec = build_grid_spec(output_degree=0.25)\n",
    "\n",
    "# note that this will error out if the mean/std isn't already saved because we're not providing a valid dataset into the function\n",
    "obs_mean = get_obs_mean(\n",
    "    obs=obs,\n",
    "    train_period_start=historical_period_start,\n",
    "    train_period_end=historical_period_end,\n",
    "    variables=variables,\n",
    "    gcm_grid_spec=gcm_grid_spec,\n",
    ")\n",
    "\n",
    "obs_std = get_obs_std(\n",
    "    obs=obs,\n",
    "    train_period_start=historical_period_start,\n",
    "    train_period_end=historical_period_end,\n",
    "    variables=variables,\n",
    "    gcm_grid_spec=gcm_grid_spec,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d36a97-1abb-49a7-8eaf-c117a42daa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled = (downscaled_norm * (obs_std[var] + EPSILON)) + obs_mean[var]\n",
    "if var == \"pr\":\n",
    "    downscaled = downscaled.clip(min=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2c0fb2-c942-4aa6-b61c-36f443cacdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_template = (\n",
    "    get_obs(\n",
    "        obs=obs,\n",
    "        train_period_start=\"1980\",\n",
    "        train_period_end=\"1980\",\n",
    "        variables=[var],\n",
    "    )\n",
    "    .isel(time=0)\n",
    "    .load()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bdd9c9f-de1b-49b0-9025-4efed8aff225",
   "metadata": {},
   "outputs": [],
   "source": [
    "# go from 720x1440 -> 721x1440\n",
    "regridder = xe.Regridder(\n",
    "    downscaled.chunk({\"time\": 1, \"lat\": -1, \"lon\": -1}),\n",
    "    obs_template,\n",
    "    \"bilinear\",\n",
    "    extrap_method=\"nearest_s2d\",\n",
    ")\n",
    "downscaled_regridded = regridder(downscaled.chunk({\"time\": 1, \"lat\": -1, \"lon\": -1})).astype(\n",
    "    \"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511a1e5c-63b8-4354-af1e-af4c526f0211",
   "metadata": {},
   "outputs": [],
   "source": [
    "downscaled_regridded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc6ed378-d9dd-49c8-9441-3ff21970fa8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fsspec.get_mapper(\n",
    "    f\"az://flow-outputs/results/deepsd/{gcm}_{scenario}_{historical_period_start}_{historical_period_end}_{inference_period_start}_{inference_period_end}_{var}.zarr\"\n",
    ")\n",
    "downscaled_regridded.astype(\"float32\").to_zarr(store, mode=\"w\", consolidated=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8512d14f-7e4e-4628-8e71-8aa831da4fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "store = fsspec.get_mapper(\n",
    "    f\"az://flow-outputs/results/deepsd/{gcm}_{scenario}_{historical_period_start}_{historical_period_end}_{inference_period_start}_{inference_period_end}_{var}.zarr\"\n",
    ")\n",
    "downscaled_regridded = xr.open_zarr(store)\n",
    "downscaled_regridded[var].isel(time=0).plot(vmin=0, vmax=15)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
