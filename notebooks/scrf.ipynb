{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3ef92da5-9e52-4238-9c95-3ae4f7c6b798",
   "metadata": {},
   "source": [
    "# Spatial-temporal Correlated Random Fields (SCRF)\n",
    "\n",
    "Author: Cindy Chiao  \n",
    "Date: 12/29/2021\n",
    "\n",
    "The purpose of this notebook is to generate spatially and temporally correlated random fields to be used to perturb the mean prediction result of GARD downscaling method. First, we use ERA5 observation data to find the appropriate length scales of correlation both spatially and temporally. Then, we use the correlation length scales to generate random fields covering the global domain in the resolution of ERA5. The package [gstools](https://geostat-framework.readthedocs.io/projects/gstools/en/stable/#pip) is used heavily in this process. A SCRF is generated for precipitation and another for temperature (will be used to perturb both tmin and tmax prediction result). \n",
    "\n",
    "Ideally, the entire available observation time series of the global domain would be used in determining the correlation length scales, and the SCRF of the entire future prediction period would be generated as one contiguous dataset. However, this proves to be prohibitive in terms of computation time due to the single threaded nature of the gstools algorithm. Thus, random subsamples of the observation time series were used to find the correlation length, and the SCRF was generated in 10 year long chunks. \n",
    "\n",
    "To find a representative spatial correlation length, we calculate the average spatial correlation lengths of 100 20x20 degree maps of 365 day time series. The 20x20 degree maps are constrained to areas of the globe that contain major landmass, avoiding the areas where it's majority ocean. \n",
    "\n",
    "To find a representative temporal correlation length, we use 10,000 samples of 365 day time series, again constrained to the areas containing major landmass. \n",
    "\n",
    "The spatial/temporal length scales for tmin and tmax are then averaged to be the length scale used to generate SCRF for temperature. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49defb15-ec4d-4c2f-900c-8a506b5f76a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "61c2d72b-e1e8-41fb-9e3c-46c74e523c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random \n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gstools as gs\n",
    "from cmip6_downscaling.workflows.paths import make_scrf_path\n",
    "from cmip6_downscaling.methods.gard import generate_scrf\n",
    "from cmip6_downscaling.data.observations import get_obs\n",
    "\n",
    "random.seed(20211228)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a833741f-09c6-48ba-bb89-1d737d3b5f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from carbonplan_trace.tiles import tiles\n",
    "from carbonplan_trace.v1 import utils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18b61376-1c7f-471d-8867-15e77b3d63ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dask\n",
    "# from dask.distributed import Client\n",
    "# from dask_gateway import Gateway\n",
    "\n",
    "# client = Client(n_workers=8)\n",
    "# client"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89fdc3e9-d724-4eaa-a917-1db1888cdcdf",
   "metadata": {},
   "source": [
    "## Finding correlation length scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0da694c9-19e5-4b66-9475-fe8bbb2e6250",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all 20x20 degree tiles that covers at least one 10x10 degree tile that has major land mass \n",
    "\n",
    "expanded_tiles = []\n",
    "for tile in tiles:\n",
    "    lat, lon = utils.get_lat_lon_tags_from_tile_path(tile)\n",
    "    min_lat, max_lat, min_lon, max_lon = utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "    \n",
    "    for i in [-1, 0]:\n",
    "        for j in [-1, 0]: \n",
    "            lat_tag, lon_tag = utils.get_lat_lon_tags_from_bounding_box(max_lat + (i * 10.), min_lon + (j * 10.))\n",
    "            expanded_tiles.append(f'{lat_tag}_{lon_tag}')\n",
    "            \n",
    "expanded_tiles = list(set(expanded_tiles))\n",
    "expanded_tiles = [t for t in expanded_tiles if '190W' not in t and '170E' not in t and '80N' not in t and '80S' not in t]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "71e34a58-fa62-401d-a89a-4f66bbc1f815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_long3_to_long1(long3):\n",
    "    # see https://confluence.ecmwf.int/pages/viewpage.action?pageId=149337515\n",
    "    long1 = (long3 + 180) % 360 - 180\n",
    "    return long1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fbc6f6d5-4c68-40b6-959f-7cb6b148b0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = ['tasmax', 'tasmin', 'pr']\n",
    "seasonality_period = 31\n",
    "temporal_scaler = 1000.0\n",
    "\n",
    "sample_length = 365\n",
    "n_samples_temporal = 10000\n",
    "n_tiles_spatial = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6a7cbaa5-0e98-4f3c-9df0-506de02a062a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spatial_length(data):\n",
    "    fields = data.values\n",
    "    print(np.mean(fields))\n",
    "    bin_center, gamma = gs.vario_estimate(\n",
    "        pos=(data.lon.values, data.lat.values),\n",
    "        field=fields,\n",
    "        latlon=True,\n",
    "        mesh_type='structured',\n",
    "    )\n",
    "    spatial = gs.Gaussian(dim=2, latlon=True, rescale=gs.EARTH_RADIUS)\n",
    "    spatial.fit_variogram(bin_center, gamma, sill=np.mean(np.var(fields, axis=(1, 2))))\n",
    "\n",
    "    return spatial.len_scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f486a486-f6a5-4cf7-8ee4-93375e18fb5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasmax\n",
      "437.36955028922074\n",
      "tasmin\n",
      "419.73912697665384\n",
      "pr\n",
      "404.29331338341586\n"
     ]
    }
   ],
   "source": [
    "# spatial length scale \n",
    "\n",
    "for v in variables:\n",
    "    print(v)\n",
    "    fname = f'{v}_spatial_length_scale.csv'\n",
    "    if os.path.exists(fname):\n",
    "        df = pd.read_csv(fname)\n",
    "        df = df.loc[df.spatial_length_scale > 1]\n",
    "        print(df.spatial_length_scale.mean())\n",
    "    else:\n",
    "        data = get_obs(\n",
    "            obs='ERA5',\n",
    "            train_period_start=1980,\n",
    "            train_period_end=2020,\n",
    "            variables=v,\n",
    "            chunking_approach=None,\n",
    "        )[v]\n",
    "\n",
    "        if v == 'pr':\n",
    "            data = data * 1e6\n",
    "\n",
    "        # go from 0-360 to -180-180 longitude \n",
    "        data['lon'] = convert_long3_to_long1(data.lon)\n",
    "        data = data.reindex(lon=sorted(data.lon.values))\n",
    "\n",
    "        # detrend \n",
    "        seasonality = (\n",
    "            data.rolling({'time': seasonality_period}, center=True, min_periods=1)\n",
    "            .mean()\n",
    "            .groupby('time.dayofyear')\n",
    "            .mean()\n",
    "        )\n",
    "        detrended = data.groupby(\"time.dayofyear\") - seasonality\n",
    "        detrended = detrended.transpose('time', 'lon', 'lat')\n",
    "        possible_time_starts = len(detrended.time) - sample_length\n",
    "\n",
    "        spatial_length_scale = []\n",
    "        chosen_tiles = random.sample(expanded_tiles, k=n_tiles_spatial)\n",
    "        for tile in chosen_tiles:\n",
    "            lat, lon = utils.get_lat_lon_tags_from_tile_path(tile)\n",
    "            min_lat, max_lat, min_lon, max_lon = utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "            max_lat += 10\n",
    "            max_lon += 10 \n",
    "            t = random.randint(a=0, b=possible_time_starts)\n",
    "            sub = detrended.sel(lat=slice(max_lat, min_lat), lon=slice(min_lon, max_lon)).isel(time=slice(t, t+sample_length))\n",
    "            # spatial_length_scale.append(client.persist(get_spatial_length(sub), retries=1))\n",
    "            l = get_spatial_length(sub)\n",
    "            spatial_length_scale.append(l)\n",
    "            print(tile, l)\n",
    "\n",
    "        df = pd.DataFrame({'tile': chosen_tiles, 'spatial_length_scale': spatial_length_scale})\n",
    "        df.to_csv(f'{v}_spatial_length_scale.csv')\n",
    "        df = df.loc[df.spatial_length_scale > 1]\n",
    "        print(df.spatial_length_scale.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "015f570b-4a3e-49fa-b68a-47981b0d3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_length = 100\n",
    "temporal_scaler = 100."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3343d4e2-066d-4381-bf25-49f41d1a9e02",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tasmax\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:1227: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detrending\n",
      "building samples\n",
      "finding correlation length\n",
      "tasmax 3.725572253986237\n",
      "tasmin\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:1227: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detrending\n",
      "building samples\n",
      "finding correlation length\n",
      "tasmin 3.9400152223556937\n",
      "pr\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/srv/conda/envs/notebook/lib/python3.9/site-packages/xarray/core/indexing.py:1227: PerformanceWarning: Slicing is producing a large chunk. To accept the large\n",
      "chunk and silence this warning, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': False}):\n",
      "    ...     array[indexer]\n",
      "\n",
      "To avoid creating the large chunks, set the option\n",
      "    >>> with dask.config.set(**{'array.slicing.split_large_chunks': True}):\n",
      "    ...     array[indexer]\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "detrending\n",
      "building samples\n",
      "finding correlation length\n",
      "pr 2.0457947933307676\n"
     ]
    }
   ],
   "source": [
    "# temporal length scale \n",
    "for v in variables:\n",
    "    print(v)\n",
    "    data = get_obs(\n",
    "        obs='ERA5',\n",
    "        train_period_start=1980,\n",
    "        train_period_end=2020,\n",
    "        variables=v,\n",
    "        chunking_approach=None,\n",
    "    )[v]\n",
    "\n",
    "    if v == 'pr':\n",
    "        data = data * 1e3\n",
    "\n",
    "    # go from 0-360 to -180-180 longitude \n",
    "    data['lon'] = convert_long3_to_long1(data.lon)\n",
    "    data = data.reindex(lon=sorted(data.lon.values))\n",
    "    \n",
    "    # detrend \n",
    "    print('detrending')\n",
    "    seasonality = (\n",
    "        data.rolling({'time': seasonality_period}, center=True, min_periods=1)\n",
    "        .mean()\n",
    "        .groupby('time.dayofyear')\n",
    "        .mean()\n",
    "    )\n",
    "    detrended = data.groupby(\"time.dayofyear\") - seasonality\n",
    "    detrended = detrended.transpose('time', 'lon', 'lat')\n",
    "    possible_time_starts = len(detrended.time) - sample_length\n",
    "    detrended = detrended.stack(point=['lat', 'lon'])\n",
    "    \n",
    "    print('building samples')\n",
    "    points = []\n",
    "\n",
    "    chosen_tiles = random.choices(tiles, k=n_samples_temporal)\n",
    "    ii = random.choices(np.arange(40), k=n_samples_temporal)\n",
    "    jj = random.choices(np.arange(40), k=n_samples_temporal)\n",
    "\n",
    "    lat_lon_tags = [utils.get_lat_lon_tags_from_tile_path(tile) for tile in chosen_tiles]\n",
    "    bounding_boxes = [\n",
    "        utils.parse_bounding_box_from_lat_lon_tags(lat, lon)\n",
    "        for lat, lon in lat_lon_tags\n",
    "    ]\n",
    "\n",
    "    for bounding_box, i, j in zip(bounding_boxes, ii, jj):\n",
    "        min_lat, max_lat, min_lon, max_lon = bounding_box\n",
    "        lat = min_lat + i * 0.25\n",
    "        lon = min_lon + j * 0.25\n",
    "        points.append((lat, lon))\n",
    "\n",
    "    points.sort(key=lambda u: u[0])\n",
    "    sub = detrended.sel(point=points).load()\n",
    "    \n",
    "    t_starts = np.array(random.choices(np.arange(possible_time_starts), k=n_samples_temporal))\n",
    "    t_ends = t_starts + sample_length\n",
    "\n",
    "    fields = []\n",
    "    for i, (start, end) in enumerate(zip(t_starts, t_ends)):\n",
    "        f = sub.isel(point=i, time=slice(start, end)).values\n",
    "        fields.append(f)\n",
    "\n",
    "    print('finding correlation length')\n",
    "    t = np.arange(sample_length) / temporal_scaler\n",
    "    bin_center, gamma = gs.vario_estimate(pos=t, field=fields, mesh_type='structured')\n",
    "    temporal = gs.Gaussian(dim=1)\n",
    "    temporal.fit_variogram(bin_center, gamma, sill=np.mean(np.var(fields, axis=1)))\n",
    "    \n",
    "    print(v, temporal.len_scale * temporal_scaler)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69b9094-a993-43c8-9f4b-4a28fac8a325",
   "metadata": {},
   "source": [
    "## Generating SRCF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b149e532-464a-4bdc-a041-c81eddf9b43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = {\n",
    "    'tasmax': 437.36955028922074,\n",
    "    'tasmin': 419.73912697665384, \n",
    "    'pr': 404.29331338341586\n",
    "}\n",
    "\n",
    "ts = {\n",
    "    'tasmax': 3.725572253986237,\n",
    "    'tasmin': 3.9400152223556937, \n",
    "    'pr': 2.0457947933307676\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f1b3d713-97b4-427c-9f2f-935657fbb155",
   "metadata": {},
   "outputs": [],
   "source": [
    "ss = 500\n",
    "ts = 10\n",
    "\n",
    "model = gs.Gaussian(dim=3, var=1.0, len_scale=[ss, ss, ts])\n",
    "srf = gs.SRF(model, seed=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23a65927-941e-49e9-bd28-318520af3847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "62.33059689203898\n"
     ]
    }
   ],
   "source": [
    "step = 25\n",
    "nx = 100\n",
    "ny = 100\n",
    "nt = 365*30+8\n",
    "\n",
    "x = np.arange(0, nx*step, step)\n",
    "y = np.arange(0, ny*step, step)\n",
    "t = np.arange(0, nt)\n",
    "\n",
    "t1 = time.time()\n",
    "field = srf.structured((x, y, t))\n",
    "t2 = time.time()\n",
    "\n",
    "print((t2-t1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9016463-a3d8-4333-a9fb-d6ba7ce5c0df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "109580000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 25\n",
    "nx = 100\n",
    "ny = 100\n",
    "nt = 365*30+8\n",
    "\n",
    "x = np.arange(0, nx*step, step)\n",
    "y = np.arange(0, ny*step, step)\n",
    "t = np.arange(0, nt)\n",
    "\n",
    "len(x) * len(y) * len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a802e03f-3ce5-4d93-aaa9-85b6befa5fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "217.89220894575118\n"
     ]
    }
   ],
   "source": [
    "# 10 year chunks \n",
    "# \n",
    "\n",
    "step = 25\n",
    "nx = 360*4\n",
    "ny = 180*4\n",
    "nt = 365\n",
    "\n",
    "x = np.arange(0, nx*step, step)\n",
    "y = np.arange(0, ny*step, step)\n",
    "t = np.arange(0, nt)\n",
    "\n",
    "t1 = time.time()\n",
    "field = srf.structured((x, y, t))\n",
    "t2 = time.time()\n",
    "\n",
    "print((t2-t1)/60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "15bd02d5-4897-4bd0-b57b-9224706baab1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db7b7111-ac54-4118-ac98-4ab087caab31",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in np.arange(30):\n",
    "    i = random.randint(0, 365)\n",
    "    plt.imshow(field[:, :, _])\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    \n",
    "# import numpy as np\n",
    "# import cv2\n",
    "# size = 720*16//9, 720\n",
    "# duration = 2\n",
    "# fps = 25\n",
    "# out = cv2.VideoWriter('output.mp4', cv2.VideoWriter_fourcc(*'mp4v'), fps, (size[1], size[0]), False)\n",
    "# for _ in range(fps * duration):\n",
    "#     data = np.random.randint(0, 256, size, dtype='uint8')\n",
    "#     out.write(data)\n",
    "# out.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0640bf65-2ee6-4546-b70d-e0b5dfc25543",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "378432000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "step = 25\n",
    "nx = 360*4\n",
    "ny = 180*4\n",
    "nt = 365\n",
    "\n",
    "x = np.arange(0, nx*step, step)\n",
    "y = np.arange(0, ny*step, step)\n",
    "t = np.arange(0, nt)\n",
    "\n",
    "len(x) * len(y) * len(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "04577b38-e612-4da7-8fb4-6ca438eeb4b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.453476911845227"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "378432000 / 109580000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c96045f-89ec-42ef-a441-571f0d78b521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4957503988475507"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "217.89220894575118 / 62.33059689203898"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c5b7a7e5-fcc3-4e24-9bbc-efb6d929f76b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "108.94610447287559"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "217.89220894575118 * 30 / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "06b51446-25c2-43ac-8522-a0ef9d8bfe57",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.DataArray(\n",
    "    np.random.rand(360*4, 180*4, 365*30+8),\n",
    "    dims=['lon', 'lat', 'time'],\n",
    "    coords=[np.arange(360*4), np.arange(180*4), np.arange(365*30+8)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de09a39-97ae-4cf9-a166-d30d40b9e21f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "90.8900352"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds.nbytes / 1e9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb04011e-3870-4f4f-8a82-efe3fecd8b79",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
