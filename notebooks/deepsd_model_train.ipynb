{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68d63005-0b6c-48b7-bba4-3fee0d0b14bc",
   "metadata": {},
   "source": [
    "# Training DeepSD models \n",
    "\n",
    "This notebook trains DeepSD models with training data prepared using `deepsd_data_prep.ipynb`. Here, it is assumed that available data is present on azure storage and separated by year and month. The model architecture and training parameters are taken from [this KDD paper](https://arxiv.org/abs/1703.03126), and the model training code is adapted from the [deepsd github repo](https://github.com/tjvandal/deepsd). We [forked the SRCNN repo implemented by the DeepSD author](https://github.com/carbonplan/srcnn-tensorflow) and made slight modifications described in this [PR](https://github.com/carbonplan/srcnn-tensorflow/pull/1). \n",
    "\n",
    "Main differences between the DeepSD model described in the paper and implemented in this notebook are: \n",
    "1. The batch normalization layer originally in the SCRNN model was removed. Instead, the input data is normalized by a 30 year historical average and standard deviation in the data prep notebook. This historical average and standard deviation is then used to restored downscaled GCM data as a form of bias correction. Note that alternatively, we can first bias correct the GCM data to the historical observation data, then normalize/restore GCM data using the historical average of the GCM data. This is not implemented for the sake of time commitment. \n",
    "2. The paper described training the model with 10^7 iterations. In this notebook, the precip model was trained with 3000 iterations and tmax/tmin models were trained with 1000 iterations. The reduced number of iterations used was mainly due to time saving, as model training was done with CPU only at this point. The difference iterations in precip and termpature model was due to the observation that, the training loss and RMSE quickly go towards 0 after training started for temperature models. My hypothesis is that this is due to the data normalization routine. Since there are pronounced seasonal trends in temperature and we use the simple 30 year historical average at each pixel for normalization, the \"normalized\" temperature data may be mostly exhibiting the seasonal trends. Thus, the data may seem \"easy\" to predict by the model based on the coarse scale data. To confirm this hypothesis, trend removal techniques (e.g. removal of long term moving average) can be implemented to make sure that the model is learning to predict the \"anomalies\" instead of the trends. A complementary hypothesis is that the fine scale elevation data used as auxilliary data is very informative in disaggregating temperature data (since there is well known relationship between elevation and temperature). If this is the case, the model may be naturally doing well and we can trust the low error metrics.  \n",
    "\n",
    "DeepSD models are stacked models of SRCNN. As an example, a DeepSD model that downscaled data from 2 degree to 0.25 degree (an 8-fold increase in resolution) would be a stacked model of 3 SRCNN models, each with a 2-fold increase in resolution (2^3 = 8). During training, each of the SRCNN model is trained independently. Then, as the last step we stack the trained models together to form the joint models. Two joint models are compiled for each variable in this notebook, one downscaling from 2 degree to 0.25 degree, and the other downscaling from 1 degree to 0.25 degree. These two models will then be used according to the initial resolution of the GCM model in question. If the original GCM model has a resolution coarser than or close to 2 degree, the GCM data will first be interpolated to 2 degree and then downscaled. If the original GCM model has a resolution closer to 1 degree, then the GCM data will first be interpolated to 1 degree and downscaled. The starting resolution of each GCM model is calculated in `deepsd_data_prep.ipynb` and manually copied to `deepsd.py` file for usage during inference. \n",
    "\n",
    "This notebook saved the model files locally then put them on azure cloud. The file names used are then copied to `deepsd.py` manually to be read during inference. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da67b2e7-b40b-47e3-b73c-d002de4709b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "894a9707-d5ef-426c-a838-8e4a2c1c1f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 21:22:07.499942: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-02-01 21:22:07.499992: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import fsspec\n",
    "import os\n",
    "import time\n",
    "import tensorflow_io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5b9ff71d-abfb-495c-b53e-d9a99a4df5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading config values \n",
    "\n",
    "import configparser\n",
    "\n",
    "config = configparser.ConfigParser()\n",
    "config.read('/home/jovyan/deepsd/config.ini')\n",
    "\n",
    "# config values that matches the DeepSD paper that we directly read from \n",
    "LAYER_SIZES = [int(k) for k in config.get('SRCNN', 'layer_sizes').split(\",\")]\n",
    "KERNEL_SIZES = [int(k) for k in config.get('SRCNN', 'kernel_sizes').split(\",\")]\n",
    "OUTPUT_DEPTH = LAYER_SIZES[-1]\n",
    "LEARNING_RATE = float(config.get('SRCNN', 'learning_rate'))\n",
    "UPSCALE_FACTOR = config.getint('DeepSD', 'upscale_factor')\n",
    "\n",
    "# how many iterations to run --> determines how long training goes \n",
    "TRAINING_ITERS = 1000  #int(config.get('SRCNN', 'training_iters'))  # paper uses 10e7 iterations/batches \n",
    "TEST_STEP = 50  #int(config.get('SRCNN', 'test_step'))\n",
    "\n",
    "# config values that differ from the DeepSD paper, and we use the values in the paper instead of in the config file \n",
    "INPUT_DEPTH = 2  # int(config.get('SRCNN', 'training_input_depth'))  # 2 instead of the 1 since we combined elevation into the input when saving data \n",
    "BATCH_SIZE = 200   #int(config.get('SRCNN', 'batch_size'))  # 200 instead of 100\n",
    "INPUT_SIZE = 51   # int(config.get('SRCNN', 'training_input_size'))  # 51 instead of 38 \n",
    "\n",
    "# where to save and get data\n",
    "az_storage_account = 'cmip6downscaling/'\n",
    "DATA_DIR = 'az://{az_storage_account}training/deepsd/{var}/{output_resolution_str}/'\n",
    "SAVE_DIR = '/home/jovyan/deepsd_models/{var}_{output_resolution_str}/'\n",
    "\n",
    "# specify training and testing years \n",
    "train_years = np.arange(1981, 2011)\n",
    "test_years = np.arange(2009, 2011)\n",
    "months = np.arange(1, 13)\n",
    "\n",
    "# variables \n",
    "variables = ['pr', 'tasmax', 'tasmin']\n",
    "output_resolutions = [0.25, 0.5, 1.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3bf9c4f7-a8b7-4cf8-a18a-ca00db06737a",
   "metadata": {},
   "outputs": [],
   "source": [
    "fs = fsspec.get_filesystem_class('az')(\n",
    "    account_name='carbonplan', account_key=os.environ['TF_AZURE_STORAGE_KEY']\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d9de8e-fa5f-4380-b791-9c2dd964c92c",
   "metadata": {},
   "source": [
    "## Individual model training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3bc3ef37-9842-42dd-b64f-777edf7a2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4dde0bbc-4fa6-40d9-a195-a9dc6e604f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/home/jovyan/srcnn-tensorflow')\n",
    "from srcnn import srcnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74d055e8-ff64-452a-8d4f-1cabf871c0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = {\n",
    "    'hr_h': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'hr_w': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'hr_d': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'lr_h': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'lr_w': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'lr_d': tf.io.FixedLenFeature([], tf.int64),\n",
    "    'label': tf.io.FixedLenFeature([], tf.string),\n",
    "    'img_in': tf.io.FixedLenFeature([], tf.string),\n",
    "    'lat': tf.io.FixedLenFeature([], tf.string),\n",
    "    'lon': tf.io.FixedLenFeature([], tf.string),\n",
    "    # TODO: this needs to be string \n",
    "    'time': tf.io.FixedLenFeature([], tf.string),\n",
    "}\n",
    "\n",
    "def read_and_decode(filename_queue, input_size, input_depth, output_depth):\n",
    "    \n",
    "    reader = tf.compat.v1.TFRecordReader()\n",
    "    _, serialized_example = reader.read(filename_queue)\n",
    "    features = tf.compat.v1.parse_single_example(\n",
    "        serialized_example,\n",
    "        features=feature)\n",
    "    \n",
    "    label = tf.io.parse_tensor(features['label'], out_type=tf.float32)\n",
    "    img_in = tf.io.parse_tensor(features['img_in'], out_type=tf.float32)\n",
    "    lat = tf.io.parse_tensor(features['lat'], out_type=tf.float32)\n",
    "    lon = tf.io.parse_tensor(features['lon'], out_type=tf.float32)\n",
    "    \n",
    "    img_in.set_shape([input_size, input_size, input_depth])\n",
    "    label.set_shape([input_size, input_size, output_depth])\n",
    "    \n",
    "    return {\"input\": img_in, \"label\": label, \"lat\":lat, \"lon\":lon}\n",
    "\n",
    "def get_inputs(filenames, batch_size, input_size, input_depth, output_depth):\n",
    "    with tf.name_scope('input'), tf.device(\"/cpu:0\"):\n",
    "        filename_queue = tf.compat.v1.train.string_input_producer(filenames)\n",
    "        data = read_and_decode(filename_queue, input_size, input_depth, output_depth)\n",
    "        \n",
    "        images, labels = tf.compat.v1.train.shuffle_batch(\n",
    "            [data['input'], data['label']], batch_size=batch_size,\n",
    "            num_threads=8, capacity=2000 + 3*batch_size,\n",
    "            min_after_dequeue=1000, allow_smaller_final_batch=True)\n",
    "    \n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa2288b8-98b2-4e60-8ecd-4c8eac4ddc6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jovyan/srcnn-tensorflow/srcnn/srcnn.py'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "srcnn.__file__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f0fe0681-9088-4f3a-908e-7ca7480e2986",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_downscaling.methods.deepsd import res_to_str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9aa7e350-f1a7-4c56-bf7a-b3ee4a99d983",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output already exists at /home/jovyan/deepsd_models/tasmax_0_25/srcnn.ckpt, skipping\n",
      "output already exists at /home/jovyan/deepsd_models/tasmin_0_25/srcnn.ckpt, skipping\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# build one model per variable x resolution \n",
    "overwrite = False\n",
    "\n",
    "for var in ['tasmax', 'tasmin']:\n",
    "    for output_resolution in [0.25]:\n",
    "        output_resolution_str = res_to_str(output_resolution)\n",
    "        data_dir = DATA_DIR.format(az_storage_account=az_storage_account, var=var, output_resolution_str=output_resolution_str)\n",
    "        save_dir = SAVE_DIR.format(var=var, output_resolution_str=output_resolution_str)\n",
    "        final_save_path = os.path.join(save_dir, \"srcnn.ckpt\")\n",
    "        if not os.path.exists(save_dir):\n",
    "            os.makedirs(save_dir)\n",
    "            \n",
    "        if os.path.exists(final_save_path + '.meta') and not overwrite:\n",
    "            print(f'output already exists at {final_save_path}, skipping')\n",
    "            continue\n",
    "        \n",
    "        print(f'starting model train for {var} {output_resolution}')\n",
    "        with tf.Graph().as_default(), tf.device(\"/cpu:0\"):    \n",
    "            # read inputs \n",
    "            train_files = [f'{data_dir}{year}-{month:02d}-zscore.tfrecords' for year in train_years for month in months]\n",
    "            test_files = [f'{data_dir}{year}-{month:02d}-zscore.tfrecords' for year in test_years for month in months]\n",
    "            train_images, train_labels = get_inputs(filenames=train_files, batch_size=BATCH_SIZE, input_size=INPUT_SIZE, input_depth=INPUT_DEPTH, output_depth=OUTPUT_DEPTH)\n",
    "            test_images, test_labels = get_inputs(filenames=test_files, batch_size=BATCH_SIZE, input_size=INPUT_SIZE, input_depth=INPUT_DEPTH, output_depth=OUTPUT_DEPTH)\n",
    "            \n",
    "            # crop the training labels\n",
    "            # the labels currently have the same spatial dimensions as the input image, but due to the convolution process, the output will be smaller than the input \n",
    "            border_size = int((sum(KERNEL_SIZES) - len(KERNEL_SIZES)) / 2)\n",
    "            train_labels_cropped = train_labels[:,border_size:-border_size,border_size:-border_size,:]\n",
    "            # the test labels are not cropped because within the srcnn code testing output is padded when `is_training` is false \n",
    "            \n",
    "            # instantiate the input x and y pipeline \n",
    "            is_training = tf.compat.v1.placeholder_with_default(True, (), name='is_training')\n",
    "            x = tf.cond(pred=is_training, true_fn=lambda: train_images, false_fn=lambda: test_images)\n",
    "            y = tf.cond(pred=is_training, true_fn=lambda: train_labels_cropped, false_fn=lambda: test_labels)\n",
    "            x = tf.identity(x, name='x')\n",
    "            y = tf.identity(y, name='y')\n",
    "            \n",
    "            # instantiate the srcnn model \n",
    "            model = srcnn.SRCNN(\n",
    "                x, y, \n",
    "                LAYER_SIZES, \n",
    "                KERNEL_SIZES, \n",
    "                is_training=is_training,\n",
    "                learning_rate=LEARNING_RATE, \n",
    "                device='/cpu:0'   # this is the line that needs to change if we want to run on gpu \n",
    "            )\n",
    "            prediction = tf.identity(model.prediction, name='prediction')\n",
    "\n",
    "            # initialize graph and start session\n",
    "            init_op = tf.group(tf.compat.v1.global_variables_initializer(),\n",
    "                               tf.compat.v1.local_variables_initializer())\n",
    "\n",
    "            sess = tf.compat.v1.Session(\n",
    "                config=tf.compat.v1.ConfigProto(allow_soft_placement=True, log_device_placement=False))\n",
    "            saver = tf.compat.v1.train.Saver()\n",
    "\n",
    "            sess.run(init_op)\n",
    "\n",
    "            # start coordinator for data\n",
    "            coord = tf.train.Coordinator()\n",
    "            threads = tf.compat.v1.train.start_queue_runners(sess=sess, coord=coord)\n",
    "\n",
    "            # summary data\n",
    "            summary_op = tf.compat.v1.summary.merge_all()\n",
    "            train_writer = tf.compat.v1.summary.FileWriter(save_dir + '/train', sess.graph)\n",
    "            test_writer = tf.compat.v1.summary.FileWriter(save_dir + '/test', sess.graph)\n",
    "            \n",
    "            # run through model training \n",
    "            for step in range(TRAINING_ITERS):\n",
    "                start_time = time.time()\n",
    "                _, train_loss, train_rmse, x_mean, pred_mean = sess.run([model.opt, model.loss, model.rmse, model.x_mean, model.pred_mean], feed_dict={is_training: True})\n",
    "                duration = time.time() - start_time\n",
    "                \n",
    "                # print(f'step {step}: train loss = {train_loss:2.5f}, train rmse = {train_rmse:2.5f}, x mean = {x_mean:2.5f}, pred mean = {pred_mean:2.5f}')\n",
    "\n",
    "                if step  % TEST_STEP == 0:\n",
    "                    train_summary = sess.run(summary_op, feed_dict={is_training: True})\n",
    "                    train_writer.add_summary(train_summary, step)\n",
    "                    test_loss, test_rmse, test_summary = sess.run([model.loss, model.rmse, summary_op], feed_dict={is_training: False})\n",
    "                    test_writer.add_summary(test_summary, step)\n",
    "                    print(\"Step: %d, Examples/sec: %0.5f, Training Loss: %2.5f, Train RMSE: %2.5f, Test RMSE: %2.5f\" % \\\n",
    "                            (step, BATCH_SIZE/duration, train_loss, train_rmse, test_rmse))\n",
    "\n",
    "            save_path = saver.save(sess, final_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8512d14f-7e4e-4628-8e71-8aa831da4fdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4a568aa-fad0-46a2-8cf2-dc23c63f411c",
   "metadata": {},
   "source": [
    "## Merge three models together "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "667dbe5f-faa2-412a-ad2e-c148ae0f3941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# these functions are adapted from deepsd inference code with minor edits \n",
    "# edits mostly involve removing code related to batch normalization and making the code tf2 compatible \n",
    "\n",
    "from tensorflow.python.framework import graph_util\n",
    "\n",
    "def freeze_graph(model_folder):\n",
    "    # We start a session and restore the graph weights\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        # We retrieve our checkpoint fullpath\n",
    "        checkpoint = tf.train.get_checkpoint_state(model_folder)\n",
    "        input_checkpoint = checkpoint.model_checkpoint_path\n",
    "\n",
    "        # We precise the file fullname of our freezed graph\n",
    "        output_graph = model_folder + \"/frozen_model.pb\"\n",
    "        if os.path.exists(output_graph):\n",
    "            os.remove(output_graph)\n",
    "\n",
    "        # Before exporting our graph, we need to precise what is our output node\n",
    "        # This is how TF decides what part of the Graph he has to keep and what part it can dump\n",
    "        # NOTE: this variable is plural, because you can have multiple output nodes\n",
    "        output_node_names = \"prediction\"\n",
    "\n",
    "        # We clear devices to allow TensorFlow to control on which device it will load operations\n",
    "        clear_devices = True\n",
    "\n",
    "        # We import the meta graph and retrieve a Saver\n",
    "        saver = tf.compat.v1.train.import_meta_graph(input_checkpoint + '.meta', clear_devices=clear_devices)\n",
    "\n",
    "        # We retrieve the protobuf graph definition\n",
    "        graph = tf.compat.v1.get_default_graph()\n",
    "        input_graph_def = graph.as_graph_def()\n",
    "        saver.restore(sess, input_checkpoint)\n",
    "        gd = sess.graph.as_graph_def()\n",
    "\n",
    "        # We use a built-in TF helper to export variables to constants\n",
    "        output_graph_def = graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            gd, # The graph_def is used to retrieve the nodes \n",
    "            output_node_names.split(\",\"), # The output node names are used to select the usefull nodes\n",
    "            variable_names_blacklist=[]\n",
    "        )\n",
    "\n",
    "        # Finally we serialize and dump the output graph to the filesystem\n",
    "        with tf.io.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(output_graph_def.node))\n",
    "\n",
    "def load_graph(frozen_graph_filename, graph_name, x):\n",
    "    # We load the protobuf file from the disk and parse it to retrieve the \n",
    "    # unserialized graph_def\n",
    "    with tf.io.gfile.GFile(frozen_graph_filename, \"rb\") as f:\n",
    "        graph_def = tf.compat.v1.GraphDef()\n",
    "        graph_def.ParseFromString(f.read())\n",
    "\n",
    "        # Then, we can use again a convenient built-in function to import a graph_def into the \n",
    "        # current default Graph\n",
    "        is_training = tf.constant(False)\n",
    "        y, = tf.import_graph_def(\n",
    "            graph_def,\n",
    "            input_map={'x': x, 'is_training': is_training},\n",
    "            return_elements=['prediction:0'],\n",
    "            name=graph_name,\n",
    "            op_dict=None,\n",
    "            producer_op_list=None\n",
    "        )\n",
    "    return y\n",
    "\n",
    "def join_graphs(checkpoints, new_checkpoint):\n",
    "    '''\n",
    "    placeholders:\n",
    "        low-resolution ppt\n",
    "        elevation for each checkpoint\n",
    "\n",
    "    x = concat([ppt, elev_1])\n",
    "    for each checkpoint:\n",
    "        x -> y\n",
    "        x = concat([y, elev_i])\n",
    "    return y\n",
    "    '''\n",
    "    # begin by freezing each graph independently\n",
    "    for cpt in checkpoints:\n",
    "        # freeze current graph\n",
    "        freeze_graph(cpt)\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "\n",
    "    x = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None, 1), name=\"lr_x\")\n",
    "    elevs = []\n",
    "    for j, cpt in enumerate(checkpoints):\n",
    "        # another elevation placeholder\n",
    "        elv = tf.compat.v1.placeholder(tf.float32, shape=(None, None, None, 1), name=\"elev_%i\" % j)\n",
    "        elevs.append(elv)\n",
    "\n",
    "        # resize low-resolution\n",
    "        h = tf.shape(input=x)[1]\n",
    "        w = tf.shape(input=x)[2]\n",
    "        size = tf.stack([h*UPSCALE_FACTOR, w*UPSCALE_FACTOR])\n",
    "        x = tf.image.resize(x, size, method=tf.image.ResizeMethod.BILINEAR)\n",
    "\n",
    "        # join elevation and interpolated image\n",
    "        x = tf.concat([x, elv], axis=3)\n",
    "        graph_name = os.path.basename(cpt.strip(\"/\"))\n",
    "\n",
    "        # load frozen graph with x as the input\n",
    "        next_input = graph_name + '/x'\n",
    "        x = load_graph(os.path.join(cpt, 'frozen_model.pb'), graph_name, x=x)\n",
    "\n",
    "    with tf.compat.v1.Session() as sess:\n",
    "        summary_op = tf.compat.v1.summary.merge_all()\n",
    "        train_writer = tf.compat.v1.summary.FileWriter(new_checkpoint, sess.graph)\n",
    "        train_writer.add_graph(tf.compat.v1.get_default_graph())\n",
    "\n",
    "        gd = sess.graph.as_graph_def()\n",
    "        output_graph = os.path.join(new_checkpoint, 'frozen_graph.pb')\n",
    "        with tf.io.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(gd.SerializeToString())\n",
    "        print(\"%d ops in the final graph.\" % len(gd.node))\n",
    "\n",
    "    tf.compat.v1.reset_default_graph()\n",
    "    return output_graph, x.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2479c71c-827c-49be-a3e0-e56de7deef05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /srv/conda/envs/notebook/lib/python3.9/site-packages/tensorflow/python/training/queue_runner_impl.py:391: QueueRunner.__init__ (from tensorflow.python.training.queue_runner_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "To construct input pipelines, use the `tf.data` module.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmax_1_0/srcnn.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-02-01 21:22:13.861691: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2022-02-01 21:22:13.861733: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-02-01 21:22:13.861760: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (jupyter-tcchiao): /proc/driver/nvidia/version does not exist\n",
      "2022-02-01 21:22:13.862242: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /tmp/ipykernel_2269/583844007.py:36: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "WARNING:tensorflow:From /srv/conda/envs/notebook/lib/python3.9/site-packages/tensorflow/python/framework/convert_to_constants.py:929: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "38 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmax_0_5/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmax_0_25/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "WARNING:tensorflow:From /tmp/ipykernel_2269/583844007.py:58: calling import_graph_def (from tensorflow.python.framework.importer) with op_dict is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please file an issue at https://github.com/tensorflow/tensorflow/issues if you depend on this feature.\n",
      "175 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmax_0_5/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmax_0_25/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "117 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmin_1_0/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmin_0_5/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmin_0_25/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "175 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmin_0_5/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "INFO:tensorflow:Restoring parameters from /home/jovyan/deepsd_models/tasmin_0_25/srcnn.ckpt\n",
      "38 ops in the final graph.\n",
      "117 ops in the final graph.\n"
     ]
    }
   ],
   "source": [
    "# save two stacked models per variable \n",
    "# one of the stacked models will consist of three layers 2.0 -> 1.0 -> 0.5 -> 0.25\n",
    "# the other will consist of two layers 1.0 -> 0.5 -> 0.25 \n",
    "# each one will be used according to the starting resolution of the GCM model \n",
    "\n",
    "resolutions = [\n",
    "    [0.25, 0.5, 1.0],\n",
    "    [0.25, 0.5],\n",
    "]\n",
    "\n",
    "output_model_files = []\n",
    "output_node_names = []\n",
    "for var in ['tasmax', 'tasmin']:\n",
    "    for resolution in resolutions:\n",
    "        resolution = sorted(resolution, reverse=True)  # make sure this is from coarse resolution to fine resolution \n",
    "        \n",
    "        # get the model names and checkpoint files in a list, also sorted from coarse resolution to fine resolution \n",
    "        model_sections = [(f'{var}_{res_to_str(res)}', res) for res in resolution]\n",
    "        CHECKPOINTS = [f\"{SAVE_DIR.format(var=var, output_resolution_str=res_to_str(res))}\" for res in resolution]\n",
    "        \n",
    "        # create the joint model name indicating the coarsening factors \n",
    "        input_res = res_to_str(UPSCALE_FACTOR * np.max(resolution))\n",
    "        output_res = res_to_str(np.min(resolution))\n",
    "        JOINED_RESOLUTION_STR = f'{input_res}d_to_{output_res}d'\n",
    "        \n",
    "        joined_checkpoint = SAVE_DIR.format(var=var, output_resolution_str=JOINED_RESOLUTION_STR)\n",
    "        if not os.path.exists(joined_checkpoint):\n",
    "            os.makedirs(joined_checkpoint)\n",
    "\n",
    "        new_graph_path, output_node_name = join_graphs(CHECKPOINTS, joined_checkpoint)\n",
    "        output_model_files.append(new_graph_path)\n",
    "        output_node_names.append(output_node_name)\n",
    "\n",
    "out = pd.DataFrame({\n",
    "    'model_file': output_model_files,\n",
    "    'output_node': output_node_names\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "74f3e032-838f-479a-8afe-64c19ddab2c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model_file</th>\n",
       "      <th>output_node</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/jovyan/deepsd_models/tasmax_2_0d_to_0_25...</td>\n",
       "      <td>tasmax_0_25/prediction:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>/home/jovyan/deepsd_models/tasmax_1_0d_to_0_25...</td>\n",
       "      <td>tasmax_0_25/prediction:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>/home/jovyan/deepsd_models/tasmin_2_0d_to_0_25...</td>\n",
       "      <td>tasmin_0_25/prediction:0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>/home/jovyan/deepsd_models/tasmin_1_0d_to_0_25...</td>\n",
       "      <td>tasmin_0_25/prediction:0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          model_file               output_node\n",
       "0  /home/jovyan/deepsd_models/tasmax_2_0d_to_0_25...  tasmax_0_25/prediction:0\n",
       "1  /home/jovyan/deepsd_models/tasmax_1_0d_to_0_25...  tasmax_0_25/prediction:0\n",
       "2  /home/jovyan/deepsd_models/tasmin_2_0d_to_0_25...  tasmin_0_25/prediction:0\n",
       "3  /home/jovyan/deepsd_models/tasmin_1_0d_to_0_25...  tasmin_0_25/prediction:0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "daafc5c4-33fc-41bf-a729-edc877abebf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "az://training/deepsd/deepsd_models/tasmax_2_0d_to_0_25d/frozen_graph.pb tasmax_0_25/prediction:0\n",
      "az://training/deepsd/deepsd_models/tasmax_1_0d_to_0_25d/frozen_graph.pb tasmax_0_25/prediction:0\n",
      "az://training/deepsd/deepsd_models/tasmin_2_0d_to_0_25d/frozen_graph.pb tasmin_0_25/prediction:0\n",
      "az://training/deepsd/deepsd_models/tasmin_1_0d_to_0_25d/frozen_graph.pb tasmin_0_25/prediction:0\n"
     ]
    }
   ],
   "source": [
    "# upload models to cloud storage\n",
    "# these paths will then be used in `deepsd_inference.ipynb`\n",
    "\n",
    "for i, row in out.iterrows():\n",
    "    local_model_path = row.model_file\n",
    "    remote_model_path = local_model_path.replace('/home/jovyan', 'az://training/deepsd')\n",
    "    print(remote_model_path, row.output_node)\n",
    "    fs.put_file(lpath=local_model_path, rpath=remote_model_path, overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a53d66ee-f737-43b2-92fb-b0292108b64e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
