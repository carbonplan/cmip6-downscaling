{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ea34acc-9562-49f4-a520-78a4ae37916b",
   "metadata": {},
   "source": [
    "# Prepare data for DeepSD models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a6f4b5-990f-4d51-8188-201f50a47079",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b15261eb-c502-4d91-b2ff-7b070c4242f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_io\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "import intake\n",
    "import dask\n",
    "import fsspec\n",
    "import xbatcher\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cmip6_downscaling.data.cmip import get_gcm, get_gcm_grid_spec\n",
    "from cmip6_downscaling.data.observations import get_obs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c125c1e-7f54-4aff-8a12-10784318a12e",
   "metadata": {},
   "source": [
    "## GCM grid to use\n",
    "\n",
    "DeepSD operates on a stacked super resolution model with 2x upscaling factor.\n",
    "This blocks of code examines the grid spacings in each GCM models and how they\n",
    "can be interpolated as a preprocessing step to match with the model that will be\n",
    "trained.\n",
    "\n",
    "Models trained on observation data will have the following resolutions:\n",
    "\n",
    "1. 2 degree -> 1 degree\n",
    "2. 1 degree -> 0.5 degree\n",
    "3. 0.5 degree -> 0.25 degree\n",
    "\n",
    "GCM data will first be interpolated to either 2 degree and 1 degree grids, then\n",
    "upscaled with either models 1-3 stacked or models 2-3 stacked.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "039ad69f-1bf0-4896-9a4a-acde9ad1d883",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_url = (\n",
    "    \"https://cmip6downscaling.blob.core.windows.net/cmip6/pangeo-cmip6.json\"\n",
    ")\n",
    "activity_ids = \"CMIP\"\n",
    "experiment_ids = \"historical\"\n",
    "member_ids = \"r1i1p1f1\"\n",
    "table_ids = \"day\"\n",
    "grid_labels = \"gn\"\n",
    "variable_ids = [\"tasmax\"]\n",
    "\n",
    "stores = intake.open_esm_datastore(col_url).search(\n",
    "    activity_id=activity_ids,\n",
    "    experiment_id=experiment_ids,\n",
    "    member_id=member_ids,\n",
    "    # source_id=source_ids,\n",
    "    table_id=table_ids,\n",
    "    grid_label=grid_labels,\n",
    "    variable_id=variable_ids,\n",
    ")\n",
    "\n",
    "all_cmip_models = stores.df.source_id.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75caa3f3-4f64-47f8-9d60-f33dc7eb0d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_resolutions = {}\n",
    "\n",
    "for gcm_name in all_cmip_models:\n",
    "    with dask.config.set(**{\"array.slicing.split_large_chunks\": False}):\n",
    "        grid_spec = get_gcm_grid_spec(gcm_name=gcm_name).split(\"_\")\n",
    "        lat_spacing = int(grid_spec[2]) / 10.0\n",
    "        lon_spacing = int(grid_spec[3]) / 10.0\n",
    "\n",
    "        starting_lat = min(round(lat_spacing), 2)\n",
    "        starting_lon = min(round(lon_spacing), 2)\n",
    "        starting_spacing = min(starting_lat, starting_lon)\n",
    "        print(\n",
    "            gcm_name.ljust(20),\n",
    "            lat_spacing,\n",
    "            lon_spacing,\n",
    "            \"interpolating to\",\n",
    "            starting_spacing,\n",
    "            \"original gcm_grid_spec\",\n",
    "            grid_spec,\n",
    "        )\n",
    "        initial_resolutions[gcm_name] = float(starting_spacing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e16e0af-4793-428f-a726-0e17793086fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this initial resolution mapping is saved to the `deepsd.py` file and later imported/used in inference\n",
    "\n",
    "initial_resolutions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b52b3dac-0f99-4a69-b345-68fae9e5750c",
   "metadata": {},
   "source": [
    "## Coarsen observation\n",
    "\n",
    "Coarsen observation from 0.25 degree to 0.5, 1, and 2 degree grids and save.\n",
    "\n",
    "There is an initial step that bilinearly interpolates from the observation grid\n",
    "(which has shape 721x1440, including lat of both 90 and -90) to a 0.25 degree\n",
    "grid (shape 720 x 1440, with the center of each grid cell being the lat/lon\n",
    "values). Then, this dataset is coarsened using the conservative method to 0.5,\n",
    "1.0, and 2.0 degree grids. Data is directly upscaled from 0.25 degrees to 0.5,\n",
    "1, and 2 degrees instead of iteratively. Finally, the coarsened data is\n",
    "bilinearly interpolated back to a 2x resolution to be used as the input features\n",
    "to the deepsd model. Note that this step is purely to upsample so that the\n",
    "features and labels of the model is of the same dimensions.\n",
    "\n",
    "That is, in order to train the deepsd model predicting 0.5 degree maps from 1.0\n",
    "degree maps, the input features went through the following steps:\n",
    "\n",
    "1. bilinear interpolation from 721x1440 grid to 720x1440 grid (0.25 degrees),\n",
    "2. conservative interpolation from 720x1440 to 180x360 grid (1.0 degrees),\n",
    "3. bilinear interpolation from 180x360 grid to 360x720 grid (0.5 degrees).\n",
    "\n",
    "This data is then saved to `make_interpolated_obs_path` with the grid spec of\n",
    "0.5 degree.\n",
    "\n",
    "The input labels went through the following steps:\n",
    "\n",
    "1. bilinear interpolation from 721x1440 grid to 720x1440 grid (0.25 degrees),\n",
    "2. conservative interpolation from 720x1440 to 360x720 grid (0.5 degrees).\n",
    "\n",
    "The features/labels will then be normalized and subset into patches as described\n",
    "in later sections.\n",
    "\n",
    "This data is then saved to `make_coarse_obs_path` with the grid spec of 0.5\n",
    "degree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41aa382d-4dca-476c-a0fb-6f9b8432ad4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xpersist import CacheStore\n",
    "from xpersist.prefect.result import XpersistResult\n",
    "from cmip6_downscaling.workflows.paths import (\n",
    "    make_coarse_obs_path,\n",
    "    make_interpolated_obs_path,\n",
    ")\n",
    "from prefect import task, Flow, Parameter\n",
    "import xesmf as xe\n",
    "from cmip6_downscaling.methods.deepsd import (\n",
    "    bilinear_interpolate,\n",
    "    conservative_interpolate,\n",
    "    build_grid_spec,\n",
    ")\n",
    "\n",
    "intermediate_cache_path = \"az://flow-outputs/intermediate\"\n",
    "intermediate_cache_store = CacheStore(intermediate_cache_path)\n",
    "serializer = \"xarray.zarr\"\n",
    "\n",
    "build_grid_spec_task = task(build_grid_spec)\n",
    "\n",
    "\n",
    "@task(\n",
    "    checkpoint=True,\n",
    "    result=XpersistResult(intermediate_cache_store, serializer=serializer),\n",
    "    target=make_coarse_obs_path,\n",
    ")\n",
    "def shift_obs_grid_task(\n",
    "    obs,\n",
    "    train_period_start,\n",
    "    train_period_end,\n",
    "    variables,\n",
    "    output_degree,\n",
    "    gcm_grid_spec,\n",
    "    chunking_approach=\"full_space\",\n",
    "):\n",
    "    output_degree = np.round(output_degree, 2)\n",
    "\n",
    "    # this task should only be used to shift the observation from 721 x 1440 (original ERA5 points) to 720 x 1440 (grid with centers)\n",
    "    assert output_degree == 0.25\n",
    "\n",
    "    # get obs in full space chunks\n",
    "    ds_obs_full_space = get_obs(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        chunking_approach=\"full_space\",\n",
    "        cache_within_rechunk=True,\n",
    "    )\n",
    "\n",
    "    # precip was in units of mm/s, which has very small numbers, changing to mm/day\n",
    "    ds_obs_full_space[\"pr\"] *= 86400\n",
    "\n",
    "    # regrid\n",
    "    return bilinear_interpolate(\n",
    "        output_degree=output_degree, ds=ds_obs_full_space\n",
    "    )\n",
    "\n",
    "\n",
    "@task(\n",
    "    checkpoint=True,\n",
    "    result=XpersistResult(intermediate_cache_store, serializer=serializer),\n",
    "    target=make_coarse_obs_path,\n",
    ")\n",
    "def coarsen_obs_task(\n",
    "    ds_obs_full_space,\n",
    "    obs,\n",
    "    train_period_start,\n",
    "    train_period_end,\n",
    "    variables,\n",
    "    output_degree,\n",
    "    gcm_grid_spec,\n",
    "    chunking_approach=\"full_space\",\n",
    "):\n",
    "    output_degree = np.round(output_degree, 2)\n",
    "\n",
    "    return conservative_interpolate(\n",
    "        output_degree=output_degree, ds=ds_obs_full_space\n",
    "    )\n",
    "\n",
    "\n",
    "@task(\n",
    "    checkpoint=True,\n",
    "    result=XpersistResult(intermediate_cache_store, serializer=serializer),\n",
    "    target=make_interpolated_obs_path,\n",
    ")\n",
    "def interpolate_obs_task(\n",
    "    ds,\n",
    "    obs,\n",
    "    train_period_start,\n",
    "    train_period_end,\n",
    "    variables,\n",
    "    output_degree,\n",
    "    gcm_grid_spec,\n",
    "    chunking_approach=\"full_space\",\n",
    "):\n",
    "    output_degree = np.round(output_degree, 2)\n",
    "\n",
    "    return bilinear_interpolate(output_degree=output_degree, ds=ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61489048-a674-4ca7-81aa-3a23133ff036",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_hyperparameters = {\n",
    "    \"OBS\": \"ERA5\",\n",
    "    \"TRAIN_PERIOD_START\": \"1981\",\n",
    "    \"TRAIN_PERIOD_END\": \"2010\",\n",
    "    \"VARIABLES\": [\"tasmax\", \"tasmin\", \"pr\"],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ed651f-34e5-4e5d-90df-26cd628617c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with Flow(name=\"prepare_obs_input_and_label\") as flow:\n",
    "    obs = Parameter(\"OBS\")\n",
    "    train_period_start = Parameter(\"TRAIN_PERIOD_START\")\n",
    "    train_period_end = Parameter(\"TRAIN_PERIOD_END\")\n",
    "    variables = Parameter(\"VARIABLES\")\n",
    "    output_degree = Parameter(\"OUTPUT_DEGREE\")\n",
    "\n",
    "    # go from 721 x 1440 to 720 x 1440\n",
    "    shifted_grid = build_grid_spec_task(output_degree=0.25)\n",
    "    shifted_grid_obs = shift_obs_grid_task(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        output_degree=0.25,\n",
    "        gcm_grid_spec=shifted_grid,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )\n",
    "\n",
    "    # coarsen to desired resolution\n",
    "    gcm_grid_spec = build_grid_spec_task(output_degree=output_degree)\n",
    "    coarse_obs = coarsen_obs_task(\n",
    "        ds_obs_full_space=shifted_grid_obs,\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        output_degree=output_degree,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )\n",
    "\n",
    "    # interpolate it back to 2x higher res\n",
    "    gcm_grid_spec = build_grid_spec_task(output_degree=output_degree / 2.0)\n",
    "    coarse_obs = interpolate_obs_task(\n",
    "        ds=coarse_obs,\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        output_degree=output_degree / 2.0,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e87edd88-f04e-4e62-b324-a9228cc3af18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for output_degree in [0.5, 1.0, 2.0]:\n",
    "    run_hyperparameters[\"OUTPUT_DEGREE\"] = output_degree\n",
    "    flow.run(parameters=run_hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28209eb8-d7ce-45ba-833a-ef34c76c85b7",
   "metadata": {},
   "source": [
    "## Also coarsen elevation\n",
    "\n",
    "Elevation data is used as auxilliary input features to the deep sd model. The\n",
    "elevation data resolution should match that of the labels. Here, we've obtained\n",
    "the elevation dataset inherent to the ERA5 observation data by dividing the\n",
    "published geopotential by gravity.\n",
    "\n",
    "This raw data also has the resolution of 721x1440. Same as the rest of the\n",
    "observation data, we first bilinearly interpolate to 720x1440, then\n",
    "conservatively interpolate to 0.5 and 1.0 degrees. The interpolated/coarsened\n",
    "data is saved to `make_coarse_elev_path` and can be read using the utility\n",
    "function `get_elevation_data`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5abc725-27ff-435b-807f-39c6a32c45da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_downscaling.methods.deepsd import (\n",
    "    make_coarse_elev_path,\n",
    "    get_elevation_data,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caa9dfc-d4c9-4ccb-b813-ae74120cc980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# raw data\n",
    "g = 9.80665\n",
    "ds = xr.open_dataset(\n",
    "    \"./e5.oper.invariant.128_129_z.ll025sc.1979010100_1979010100.nc\"\n",
    ")\n",
    "elev = ds.Z[0] / g\n",
    "elev.plot(robust=True)\n",
    "\n",
    "# reformat\n",
    "elev = elev.drop(\"time\").rename(\"elevation\").to_dataset()\n",
    "elev = elev.rename({\"latitude\": \"lat\", \"longitude\": \"lon\"})\n",
    "elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26fb469f-8023-40db-91dd-81887d55c313",
   "metadata": {},
   "outputs": [],
   "source": [
    "# first bilinearly interpolate to a 0.25 degree grid\n",
    "output_degree = 0.25\n",
    "output_path = make_coarse_elev_path(output_degree)\n",
    "store = fsspec.get_mapper(output_path)\n",
    "\n",
    "if \".zmetadata\" not in store:\n",
    "    elev_regridded = bilinear_interpolate(output_degree=output_degree, ds=elev)\n",
    "    elev_regridded.to_zarr(store, mode=\"w\", consolidated=True)\n",
    "else:\n",
    "    elev_regridded = xr.open_zarr(store)\n",
    "\n",
    "# then interpolate the 0.25 grid into 0.5 and 1.0 with conservative method\n",
    "elev_regridded.load()\n",
    "for output_degree in [0.5, 1.0]:\n",
    "    output_path = make_coarse_elev_path(output_degree)\n",
    "    store = fsspec.get_mapper(output_path)\n",
    "    if \".zmetadata\" not in store:\n",
    "        ds_regridded = conservative_interpolate(\n",
    "            output_degree=output_degree, ds=elev_regridded\n",
    "        )\n",
    "        ds_regridded.to_zarr(store, mode=\"w\", consolidated=True)\n",
    "    else:\n",
    "        ds_regridded = xr.open_zarr(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d745c891-f34b-4b06-b989-5fd007aa5faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inspect the elevation data to use as land mask\n",
    "\n",
    "# use abs elevation > 1 as land mask\n",
    "# only saving patches where > 50% is land\n",
    "np.abs(elev.elevation > 1).astype(int).plot(robust=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6405baaf-43e7-4d6a-ba52-09dbcd0e902e",
   "metadata": {},
   "source": [
    "## Visualilze data together\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c5703f4-9cea-40de-bf7e-4b73b4fda3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(path):\n",
    "    store = fsspec.get_mapper(intermediate_cache_path + \"/\" + path)\n",
    "    return xr.open_zarr(store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48ddafa-28e1-4dc0-a273-51b5225b517e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "time = 100\n",
    "lat = slice(30, 50)\n",
    "lon = slice(-130, -100)\n",
    "\n",
    "obs = run_hyperparameters[\"OBS\"]\n",
    "train_period_start = run_hyperparameters[\"TRAIN_PERIOD_START\"]\n",
    "train_period_end = run_hyperparameters[\"TRAIN_PERIOD_END\"]\n",
    "variables = run_hyperparameters[\"VARIABLES\"]\n",
    "\n",
    "for var in [\"tasmax\", \"pr\"]:\n",
    "    for output_degree in [0.25, 0.5, 1.0]:\n",
    "        print(\n",
    "            f\"for predicting {output_degree} degree from {output_degree*2} degree maps\"\n",
    "        )\n",
    "        gcm_grid_spec = build_grid_spec(output_degree=output_degree)\n",
    "        label_path = make_coarse_obs_path(\n",
    "            obs=obs,\n",
    "            train_period_start=train_period_start,\n",
    "            train_period_end=train_period_end,\n",
    "            variables=variables,\n",
    "            gcm_grid_spec=gcm_grid_spec,\n",
    "            chunking_approach=\"full_space\",\n",
    "        )\n",
    "        ds_obs_label = get_data(label_path)\n",
    "\n",
    "        feature_path = make_interpolated_obs_path(\n",
    "            obs=obs,\n",
    "            train_period_start=train_period_start,\n",
    "            train_period_end=train_period_end,\n",
    "            variables=variables,\n",
    "            gcm_grid_spec=gcm_grid_spec,\n",
    "            chunking_approach=\"full_space\",\n",
    "        )\n",
    "        ds_obs_feature = get_data(feature_path)\n",
    "\n",
    "        ds_elev = get_elevation_data(output_degree)\n",
    "\n",
    "        plt.figure(figsize=(17, 4))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        ds_obs_feature[var].isel(time=time).sel(lat=lat, lon=lon).plot(\n",
    "            robust=True, ax=plt.gca()\n",
    "        )\n",
    "        plt.title(\"features\")\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        ds_elev.elevation.sel(lat=lat, lon=lon).plot(robust=True, ax=plt.gca())\n",
    "        plt.title(\"elevation\")\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        ds_obs_label[var].isel(time=time).sel(lat=lat, lon=lon).plot(\n",
    "            robust=True, ax=plt.gca()\n",
    "        )\n",
    "        plt.title(\"label\")\n",
    "        plt.show()\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece020f1-f6e7-470d-8504-9af24142ec88",
   "metadata": {},
   "source": [
    "## Create patches for training\n",
    "\n",
    "In the deepsd setup, the models are trained with \"patches\" of the data, instead\n",
    "of the entire map at the same time. This block of code generates these patches\n",
    "using xbatcher, and save the resulting data in tfrecord format for later usage.\n",
    "\n",
    "The patches used for training is 51 x 51 in the spatial dimensions (lat/lon or\n",
    "x/y), and contains 2 layers (1 for the coarsened/interpolated image, 1 for fine\n",
    "scale elevation). The labels are the same spatial patch from the fine scale\n",
    "image. This process is repeated for each level of model.\n",
    "\n",
    "The data in the patches are normalized (z-scored) using a historical period\n",
    "defined by train_period_start and train_period_end. At the time of writing this\n",
    "is set to 1981-2010.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd40f39-cef0-4ff5-8629-b7f5ece10100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the next three functions are copied from tensorflow documentation, only used when serializing the data from regular numpy to tfrecord\n",
    "def _bytes_feature(value):\n",
    "    \"\"\"Returns a bytes_list from a string / byte.\"\"\"\n",
    "    if isinstance(value, type(tf.constant(0))):\n",
    "        value = (\n",
    "            value.numpy()\n",
    "        )  # BytesList won't unpack a string from an EagerTensor.\n",
    "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
    "\n",
    "\n",
    "def _float_feature(value):\n",
    "    \"\"\"Returns a float_list from a float / double.\"\"\"\n",
    "    return tf.train.Feature(float_list=tf.train.FloatList(value=[value]))\n",
    "\n",
    "\n",
    "def _int64_feature(value):\n",
    "    \"\"\"Returns an int64_list from a bool / enum / int / uint.\"\"\"\n",
    "    return tf.train.Feature(int64_list=tf.train.Int64List(value=[value]))\n",
    "\n",
    "\n",
    "# the next two functions are wrappers used for converting patches to serialized tensorflow records\n",
    "def serialize_example(img_in, label, lat, lon, time):\n",
    "    \"\"\"\n",
    "    Creates a tf.train.Example message ready to be written to a file.\n",
    "    \"\"\"\n",
    "    # Create a dictionary mapping the feature name to the tf.train.Example-compatible\n",
    "    # data type.\n",
    "    # hr = high resolution, lr = low resolution\n",
    "    # h = height, w = width, d = depth\n",
    "    hr_h, hr_w, hr_d = img_in.shape\n",
    "    lr_h, lr_w, lr_d = label.shape\n",
    "\n",
    "    feature = {\n",
    "        \"hr_h\": _int64_feature(hr_h),\n",
    "        \"hr_w\": _int64_feature(hr_w),\n",
    "        \"hr_d\": _int64_feature(hr_d),\n",
    "        \"lr_h\": _int64_feature(lr_h),\n",
    "        \"lr_w\": _int64_feature(lr_w),\n",
    "        \"lr_d\": _int64_feature(lr_d),\n",
    "        \"label\": _bytes_feature(tf.io.serialize_tensor(label)),\n",
    "        \"img_in\": _bytes_feature(tf.io.serialize_tensor(img_in)),\n",
    "        \"lat\": _bytes_feature(tf.io.serialize_tensor(lat)),\n",
    "        \"lon\": _bytes_feature(tf.io.serialize_tensor(lon)),\n",
    "        # TODO: this needs to be string\n",
    "        \"time\": _bytes_feature(str(time).encode(\"utf-8\")),\n",
    "    }\n",
    "\n",
    "    # Create a Features message using tf.train.Example.\n",
    "    example_proto = tf.train.Example(\n",
    "        features=tf.train.Features(feature=feature)\n",
    "    )\n",
    "    return example_proto.SerializeToString()\n",
    "\n",
    "\n",
    "def convert_to_tf(filename, inputs, labels, lats, lons, times, elevs):\n",
    "    # change all datatype to float32\n",
    "    inputs = inputs.astype(np.float32)\n",
    "    labels = labels.astype(np.float32)\n",
    "    lats = lats.astype(np.float32)\n",
    "    lons = lons.astype(np.float32)\n",
    "\n",
    "    with tf.io.TFRecordWriter(filename) as writer:\n",
    "        total_in = inputs.shape[0]\n",
    "        land = 0\n",
    "        nans = 0\n",
    "        # loop through each patch and serialize/write them\n",
    "        for i, img_in in enumerate(inputs):\n",
    "            # only saving patches where > 30% is land\n",
    "            if np.mean(np.abs(elevs[i, :, :, :]) < 1) > 0.7:\n",
    "                continue\n",
    "\n",
    "            if not np.isfinite(img_in).all():\n",
    "                nans += 1\n",
    "                continue\n",
    "            example = serialize_example(\n",
    "                img_in, labels[i], lats[i], lons[i], times[i][0]\n",
    "            )\n",
    "            writer.write(example)\n",
    "            land += 1\n",
    "        print(\n",
    "            f\"{total_in} input patches, saving {land} that are mostly land, {nans} nans\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43e2015-b968-4762-873b-a4896f09de19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note that the deepsd code on github shows an input size of 38 in the config file https://github.com/tjvandal/deepsd/blob/1978773b41141347aedc3c689e2297b024ef71bb/config-large.ini#L19\n",
    "# however, the deepsd paper indicated that training is done with 51x51 patches at a stride of 20 https://arxiv.org/pdf/1703.03126.pdf\n",
    "# in general, we go with the parameters set by the paper here\n",
    "\n",
    "from cmip6_downscaling.methods.deepsd import INPUT_SIZE, PATCH_STRIDE\n",
    "\n",
    "sample_size = {\"lat\": INPUT_SIZE, \"lon\": INPUT_SIZE, \"time\": 1, \"variable\": 8}\n",
    "stride = PATCH_STRIDE\n",
    "input_overlap = {\n",
    "    \"lat\": sample_size[\"lat\"] - stride,\n",
    "    \"lon\": sample_size[\"lon\"] - stride,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f8aa2d0-7909-4476-9635-a4bd58eba88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import fsspec\n",
    "\n",
    "fs = fsspec.get_filesystem_class(\"az\")(\n",
    "    account_name=\"carbonplan\", account_key=os.environ[\"TF_AZURE_STORAGE_KEY\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec43a76-0d6b-4f72-92de-0ddc514e3088",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from cmip6_downscaling.methods.deepsd import (\n",
    "    EPSILON,\n",
    "    get_obs_mean,\n",
    "    get_obs_std,\n",
    "    normalize,\n",
    ")\n",
    "\n",
    "# for output_degree in [0.25, 0.5, 1.0]:\n",
    "for output_degree in [0.25]:\n",
    "    print(\n",
    "        f\"for predicting {output_degree} degree from {output_degree*2} degree maps\"\n",
    "    )\n",
    "    gcm_grid_spec = build_grid_spec(output_degree=output_degree)\n",
    "\n",
    "    print(\"label\")\n",
    "    # loading label data\n",
    "    label_path = make_coarse_obs_path(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )\n",
    "    ds_obs_label = get_data(label_path)\n",
    "\n",
    "    # calculate mean and std for normalizing/zscoring\n",
    "    ds_obs_label_mean = get_obs_mean(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_label,\n",
    "    )\n",
    "    ds_obs_label_std = get_obs_std(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_label,\n",
    "    )\n",
    "    ds_obs_label = (ds_obs_label - ds_obs_label_mean) / (\n",
    "        ds_obs_label_std + EPSILON\n",
    "    )\n",
    "\n",
    "    print(\"feature\")\n",
    "    # get feature data\n",
    "    feature_path = make_interpolated_obs_path(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )\n",
    "    ds_obs_feature = get_data(feature_path)\n",
    "\n",
    "    # calculate mean and std for normalizing/zscoring\n",
    "    ds_obs_feature_mean = get_obs_mean(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_feature,\n",
    "    )\n",
    "    ds_obs_feature_std = get_obs_std(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_feature,\n",
    "    )\n",
    "    ds_obs_feature = (ds_obs_feature - ds_obs_feature_mean) / (\n",
    "        ds_obs_feature_std + EPSILON\n",
    "    )\n",
    "\n",
    "    print(\"elev\")\n",
    "    # load and normalize elevation data\n",
    "    ds_elev = get_elevation_data(output_degree)\n",
    "    ds_elev_norm = normalize(ds=ds_elev, dims=[\"lat\", \"lon\"], epsilon=EPSILON)\n",
    "\n",
    "    # chunk elevation data so that it's spatially contiguous\n",
    "    # and put all the features/label data into the same dataset, this is so that when using xbatcher to generate patches, all variables can be subsetted at the same time\n",
    "    ds_obs_feature[\"elevation\"] = ds_elev_norm[\"elevation\"].chunk(\n",
    "        {\"lat\": -1, \"lon\": -1}\n",
    "    )\n",
    "    for var in [\"tasmax\", \"tasmin\", \"pr\"]:\n",
    "        ds_obs_feature[var + \"_label\"] = ds_obs_label[var]\n",
    "    # the un-normalized elevation is also passed into the patch generation code, but is only used to filter out water patches\n",
    "    ds_obs_feature[\"elevation_original\"] = ds_elev[\"elevation\"].chunk(\n",
    "        {\"lat\": -1, \"lon\": -1}\n",
    "    )\n",
    "\n",
    "    # convert into an array such that \"variable\" becomes a dimension\n",
    "    ds_obs_feature = ds_obs_feature.to_array(name=\"tf\")\n",
    "\n",
    "    # loop through each year/month and generate batches for each month\n",
    "    for year, ds_year in ds_obs_feature.groupby(\"time.year\"):\n",
    "        if year >= 2009:\n",
    "            for month, ds_month in ds_year.groupby(\"time.month\"):\n",
    "                print(f\"processing {year}-{month:02d}\")\n",
    "\n",
    "                bgen = xbatcher.BatchGenerator(\n",
    "                    ds=ds_month,\n",
    "                    input_dims=sample_size,\n",
    "                    concat_input_dims=True,\n",
    "                    input_overlap=input_overlap,\n",
    "                    preload_batch=True,\n",
    "                )\n",
    "                # even though there's a for loop here, there's really only one batch in the generator\n",
    "                # we are only uing xbatcher to create the patches and not using the batching capability\n",
    "                for j, batch in enumerate(bgen):\n",
    "                    batch = batch.transpose(\n",
    "                        \"input_batch\",\n",
    "                        \"lat_input\",\n",
    "                        \"lon_input\",\n",
    "                        \"variable_input\",\n",
    "                        \"time_input\",\n",
    "                    ).load()\n",
    "                    for var in [\"pr\", \"tasmax\", \"tasmin\"]:\n",
    "                        # for var in ['tasmax']:\n",
    "                        print(\"variable\", var)\n",
    "                        # grab the correct inputs for each variable and get the numpy versions of data\n",
    "                        input_vars = [\n",
    "                            np.argwhere(batch.variable.values == var)[0][0]\n",
    "                        ] + [\n",
    "                            np.argwhere(batch.variable.values == \"elevation\")[\n",
    "                                0\n",
    "                            ][0]\n",
    "                        ]\n",
    "                        label_var = [\n",
    "                            np.argwhere(\n",
    "                                batch.variable.values == var + \"_label\"\n",
    "                            )[0][0]\n",
    "                        ]\n",
    "                        elev_var = [\n",
    "                            np.argwhere(\n",
    "                                batch.variable.values == \"elevation_original\"\n",
    "                            )[0][0]\n",
    "                        ]\n",
    "\n",
    "                        inputs = (\n",
    "                            batch.isel(variable_input=input_vars)\n",
    "                            .squeeze(\"time_input\")\n",
    "                            .tf.values\n",
    "                        )\n",
    "                        labels = (\n",
    "                            batch.isel(variable_input=label_var)\n",
    "                            .squeeze(\"time_input\")\n",
    "                            .tf.values\n",
    "                        )\n",
    "                        elevs = (\n",
    "                            batch.isel(variable_input=elev_var)\n",
    "                            .squeeze(\"time_input\")\n",
    "                            .tf.values\n",
    "                        )\n",
    "                        lats = batch.lat.values\n",
    "                        lons = batch.lon.values\n",
    "                        times = batch.time.values\n",
    "\n",
    "                        d = str(output_degree).replace(\".\", \"_\")\n",
    "                        filename = f\"az://cmip6downscaling/training/deepsd/{var}/{d}/{year}-{month:02d}-zscore.tfrecords\"\n",
    "\n",
    "                        fn_wo_container = f\"az://training/deepsd/{var}/{d}/{year}-{month:02d}-zscore.tfrecords\"\n",
    "                        # if fs.exists(fn_wo_container):\n",
    "                        #     print(f'{filename} already exists, skipping')\n",
    "                        # else:\n",
    "                        print(f\"{filename} processing\")\n",
    "                        convert_to_tf(\n",
    "                            filename, inputs, labels, lats, lons, times, elevs\n",
    "                        )\n",
    "\n",
    "                    del batch, inputs, labels, lats, lons, times, ds_month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1353de67-4d5b-4bf2-94ff-9d6e0c6ca019",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "# for plotting with lat/lon potentially use the following\n",
    "# batch.isel(input_batch=1).tf..plot.pcolormesh(strings)\n",
    "for _ in range(10):\n",
    "\n",
    "    i = random.randint(0, len(batch.input_batch))\n",
    "    patch = batch.isel(input_batch=i)\n",
    "    print(patch.lat.min().values, patch.lat.max().values)\n",
    "    print(patch.lon.min().values, patch.lon.max().values)\n",
    "    test_vals = patch.tf.values\n",
    "\n",
    "    plt.figure(figsize=(17, 4))\n",
    "\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.imshow(test_vals[:, :, 0], vmin=0, vmax=20)\n",
    "    plt.title(\"features\")\n",
    "\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(test_vals[:, :, 1], vmin=0, vmax=1500)\n",
    "    plt.title(\"elevation\")\n",
    "\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow(test_vals[:, :, 2], vmin=0, vmax=20)\n",
    "    plt.title(\"label\")\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72da81b1-a285-4d40-8d74-9a3f3c4ad255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from cmip6_downscaling.methods.deepsd import (\n",
    "    EPSILON,\n",
    "    get_obs_mean,\n",
    "    get_obs_std,\n",
    "    normalize,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "251e9d0f-10d1-46b9-8bf4-884868a6a0eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "for output_degree in [1.0]:\n",
    "    print(\n",
    "        f\"for predicting {output_degree} degree from {output_degree*2} degree maps\"\n",
    "    )\n",
    "    gcm_grid_spec = build_grid_spec(output_degree=output_degree)\n",
    "\n",
    "    print(\"label\")\n",
    "    # loading label data\n",
    "    label_path = make_coarse_obs_path(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )\n",
    "    ds_obs_label = get_data(label_path)\n",
    "\n",
    "    # calculate mean and std for normalizing/zscoring\n",
    "    ds_obs_label_mean = get_obs_mean(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_label,\n",
    "    )\n",
    "    ds_obs_label_std = get_obs_std(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_label,\n",
    "    )\n",
    "    ds_obs_label = (ds_obs_label - ds_obs_label_mean) / (\n",
    "        ds_obs_label_std + EPSILON\n",
    "    )\n",
    "\n",
    "    print(\"feature\")\n",
    "    # get feature data\n",
    "    feature_path = make_interpolated_obs_path(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        chunking_approach=\"full_space\",\n",
    "    )\n",
    "    ds_obs_feature = get_data(feature_path)\n",
    "\n",
    "    # calculate mean and std for normalizing/zscoring\n",
    "    ds_obs_feature_mean = get_obs_mean(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_feature,\n",
    "    )\n",
    "    ds_obs_feature_std = get_obs_std(\n",
    "        obs=obs,\n",
    "        train_period_start=train_period_start,\n",
    "        train_period_end=train_period_end,\n",
    "        variables=variables,\n",
    "        gcm_grid_spec=gcm_grid_spec,\n",
    "        ds=ds_obs_feature,\n",
    "    )\n",
    "    ds_obs_feature = (ds_obs_feature - ds_obs_feature_mean) / (\n",
    "        ds_obs_feature_std + EPSILON\n",
    "    )\n",
    "\n",
    "    print(\"elev\")\n",
    "    # load and normalize elevation data\n",
    "    ds_elev = get_elevation_data(output_degree)\n",
    "    ds_elev_norm = normalize(ds=elev, dims=[\"lat\", \"lon\"], epsilon=EPSILON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c49456-aa75-4037-9c9e-c37ce8ce4992",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(ds_obs_feature.pr).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmax).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmin).all().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "311ed241-8c45-446e-a58e-14d163e5d63f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obs_feature[\"elevation\"] = ds_elev_norm[\"elevation\"].chunk(\n",
    "    {\"lat\": -1, \"lon\": -1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234b75de-2e5b-4fb4-8cb1-e3499b47ea05",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_elev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd56db70-e28e-41a6-931c-ec0529827a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_elev_norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82d5e822-c7fd-4a31-9246-1f92c868789c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(ds_obs_feature.pr).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmax).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmin).all().values)\n",
    "print(np.isfinite(ds_obs_feature.elevation).all().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d95ca8b5-c6d8-47e6-99ed-2b747e898e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "for var in [\"tasmax\", \"tasmin\", \"pr\"]:\n",
    "    ds_obs_feature[var + \"_label\"] = ds_obs_label[var]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96225bd0-f72b-4509-8e96-5377ed3550bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(ds_obs_feature.pr_label).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmax_label).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmin_label).all().values)\n",
    "print(np.isfinite(ds_obs_feature.elevation).all().values)\n",
    "print(np.isfinite(ds_obs_feature.pr).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmax).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmin).all().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f07e25c-85dc-4fba-be76-cef0bab518f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_obs_feature[\"elevation_original\"] = ds_elev[\"elevation\"].chunk(\n",
    "    {\"lat\": -1, \"lon\": -1}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49b8abf9-6c1e-4be3-ab38-49775df8a710",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.isfinite(ds_obs_feature.pr_label).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmax_label).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmin_label).all().values)\n",
    "print(np.isfinite(ds_obs_feature.elevation).all().values)\n",
    "print(np.isfinite(ds_obs_feature.pr).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmax).all().values)\n",
    "print(np.isfinite(ds_obs_feature.tasmin).all().values)\n",
    "print(np.isfinite(ds_obs_feature.elevation_original).all().values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b42b30-f6ea-46f1-979f-3555188cfa2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert into an array such that \"variable\" becomes a dimension\n",
    "ds_obs_feature = ds_obs_feature.to_array(name=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3737b6b6-2ea8-408c-be42-853369676795",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.isfinite(ds_obs_feature).all().values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4604548-117b-454b-95eb-b74cd28a3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "for year, ds_year in ds_obs_feature.groupby(\"time.year\"):\n",
    "    for month, ds_month in ds_year.groupby(\"time.month\"):\n",
    "        print(f\"processing {year}-{month:02d}\")\n",
    "\n",
    "        bgen = xbatcher.BatchGenerator(\n",
    "            ds=ds_month,\n",
    "            input_dims=sample_size,\n",
    "            concat_input_dims=True,\n",
    "            input_overlap=input_overlap,\n",
    "            preload_batch=True,\n",
    "        )\n",
    "        # even though there's a for loop here, there's really only one batch in the generator\n",
    "        # we are only uing xbatcher to create the patches and not using the batching capability\n",
    "        for j, batch in enumerate(bgen):\n",
    "            break\n",
    "\n",
    "        break\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bd0b5f2-610b-44d0-bd84-7eb2fb566d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = batch.transpose('input_batch', 'lat_input', 'lon_input', 'variable_input', 'time_input').load()\n",
    "            for var in ['pr', 'tasmax', 'tasmin']:\n",
    "            # for var in ['tasmax']:\n",
    "                print('variable', var)\n",
    "                # grab the correct inputs for each variable and get the numpy versions of data \n",
    "                input_vars = [np.argwhere(batch.variable.values == var)[0][0]] + [np.argwhere(batch.variable.values == 'elevation')[0][0]]\n",
    "                label_var = [np.argwhere(batch.variable.values == var + '_label')[0][0]]\n",
    "                elev_var = [np.argwhere(batch.variable.values == 'elevation_original')[0][0]]\n",
    "\n",
    "                inputs = batch.isel(variable_input=input_vars).squeeze('time_input').tf.values\n",
    "                labels = batch.isel(variable_input=label_var).squeeze('time_input').tf.values\n",
    "                elevs = batch.isel(variable_input=elev_var).squeeze('time_input').tf.values\n",
    "                lats = batch.lat.values\n",
    "                lons = batch.lon.values\n",
    "                times = batch.time.values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
